Name           : Layer          (n, c, h, w)         (s_w, s_h, p_w, p_h)

input          : Input          
conv1          : Convolution    (96, 3, 11, 11)      (4, 4, 0, 0)
relu1          : ReLU           
norm1          : LRN            
pool1          : Pooling        (type: MAX, 3, 3)    (2, 2, 0, 0)
conv2          : Convolution    (256, 48, 5, 5)      (1, 1, 2, 2)
relu2          : ReLU           
norm2          : LRN            
pool2          : Pooling        (type: MAX, 3, 3)    (2, 2, 0, 0)
conv3          : Convolution    (384, 256, 3, 3)     (1, 1, 1, 1)
relu3          : ReLU           
conv4          : Convolution    (384, 192, 3, 3)     (1, 1, 1, 1)
relu4          : ReLU           
conv5          : Convolution    (256, 192, 3, 3)     (1, 1, 1, 1)
relu5          : ReLU           
pool5          : Pooling        (type: MAX, 3, 3)    (2, 2, 0, 0)
fc6            : InnerProduct   (4096, 9216)        
relu6          : ReLU           
drop6          : Dropout        
fc7            : InnerProduct   (4096, 4096)        
relu7          : ReLU           
drop7          : Dropout        
fc8            : InnerProduct   (1000, 4096)        
prob           : Softmax        


FeatureMaps    :  (b, c, h, w)

data           :  (1, 3, 227, 227)
conv1          :  (1, 96, 55, 55)
norm1          :  (1, 96, 55, 55)
pool1          :  (1, 96, 27, 27)
conv2          :  (1, 256, 27, 27)
norm2          :  (1, 256, 27, 27)
pool2          :  (1, 256, 13, 13)
conv3          :  (1, 384, 13, 13)
conv4          :  (1, 384, 13, 13)
conv5          :  (1, 256, 13, 13)
pool5          :  (1, 256, 6, 6)
fc6            :  (1, 4096)
fc7            :  (1, 4096)
fc8            :  (1, 1000)
prob           :  (1, 1000)
